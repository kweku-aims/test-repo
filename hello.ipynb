{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "^C\n"
                }
            ],
            "source": "!pip install fastai==2.4"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'fastai'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
                        "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3536/3197005475.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fastai'"
                    ]
                }
            ],
            "source": "import numpy as np\nimport pandas as pd\n\n# from fastai.vision.all import *\n"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "FileNotFoundError",
                    "evalue": "[Errno 2] No such file or directory: 'Desktop/powerbi/imports-85.csv'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/wsuser/ipykernel_164/659680330.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Desktop/powerbi/imports-85.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m headers=['symboling','normalized-losses','make','fuel-type','aspiration','num-of-doors','body-style','drive-wheels','engine-location',\n\u001b[1;32m     21\u001b[0m          \u001b[0;34m'wheel-base'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'length'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'width'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'height'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'curb-weight'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'engine-type'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'numofcylinders'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.9/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.9/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.9/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.9/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.9/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.9/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.9/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.9/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Desktop/powerbi/imports-85.csv'"
                    ]
                }
            ],
            "source": "from cProfile import label\nfrom dataclasses import replace\nfrom statistics import mean\nfrom textwrap import fill\nfrom tracemalloc import Snapshot\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\ndf=pd.read_csv('Desktop/powerbi/imports-85.csv')\nheaders=['symboling','normalized-losses','make','fuel-type','aspiration','num-of-doors','body-style','drive-wheels','engine-location',\n         'wheel-base','length','width','height','curb-weight','engine-type','numofcylinders',\n         'engine-size','fuel-system','bore','stroke','compression-ratio','horsepower',\n         'peak-rpm','citympg','highway-mpg','price']\ndf.columns=headers\n\npath =('Desktop/automobile.csv')\ndf.to_csv(path)\ndf=pd.read_csv('Desktop/automobile.csv')\n#df.dropna(subset=['price'],axis=0,inplace=True)\n#df.dropna(subset=['normalized-losses'],axis=0,inplace=True)\ndf['normalized-losses'].replace('?',0,inplace=True)\n#df['engine-size'].replace('?',0,inplace=True)\ndf['horsepower'].replace('?',0,inplace=True)\ndf['price'].replace('?',0,inplace=True)\ndf['citympg']=235/df['citympg']\ndf.rename(columns={'citympg':'cityL/100km'},inplace=True)\ndf['price']=df['price'].astype('int')\ndf['horsepower']=df['horsepower'].astype('int')\n\n#Normalising the Length dataset\ndf['length']=(df['length']-df['length'].mean())/df['length'].std()\n#print(df['length'].std())\n\n#Binning the price dataset\nbins =np.linspace(min(df['price']),max(df['price']),4)\ngroup_names=['low','medium','high']\ndf['price-binned']=pd.cut(df['price'],bins,labels=group_names,include_lowest=True)\n\n#Assigning dummy variables for categorical dataset\n\npd.get_dummies(df['fuel-type'])\n\n#Descriptive Statistics:Summary of categorical variables with value_counts()\ndrive_wheels_counts=df['drive-wheels'].value_counts()\n\n#we put it into a column\ndf.rename(columns={'drive-wheels':'drive_wheels_counts'},inplace=True)\ndrive_wheels_counts.index.name='drive-wheels'\n\n#Box Plots for easy comparison between groups\n#sns.boxplot(x='drive_wheels_counts',y='price', data=df)\n\n#Scatterplots for continuous variables, relationship between two variables\n#y=df['engine-size']\n#x=df['price']\n#plt.scatter(x,y)\n#plt.title('Relationship between price and engine size')\n#plt.xlabel('price')\n#plt.ylabel('engine-size')\n\n#Grouping with Groupby; for categorical variables\ndf_test=df[['drive_wheels_counts','body-style','price']]\ndf_grp=df_test.groupby(['drive_wheels_counts','body-style'],as_index=False).mean()\ndf_pivot=df_grp.pivot(index='drive_wheels_counts',columns='body-style')\n\n#print(df_pivot)\n\n#HeatMaps are a great way to plot the target variable over multiple variables to get a visualisation\n#of the relationship between these variables and the target variable\n\n#plt.pcolor(df_pivot,cmap='RdBu')\n#plt.colorbar()\n#plt.show()\n\n#Correlation Plots\n#sns.regplot(x='engine-size',y='price',data=df)\n#plt.ylim(0,)\n\n#Linear Regression\nlm=LinearRegression()\n\n#Define predictor and target variables\n#X=df[['highway-mpg']]\n#Y=df['price']\n#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n#Training Our Model\n#lm.fit(X,Y)\n#Yhat=lm.predict(X)\n#lm.coef_\n#lm.intercept_\n#print(lm.score(X, Y))\n\n#Exploring Our Results\n\n#lm.fit(Y.to_numpy().reshape(-1,1),Yhat)\n#residual = Yhat - lm.predict(Y.to_numpy().reshape(-1,1))\n#sns.set(style=\"whitegrid\")\n#fig, ax = plt.subplots(figsize =(5,5))\n#sns.regplot(x=Yhat,y=Y-Yhat,ax=ax,lowess=True)\n#ax.set(ylabel='residuals',xlabel='fitted values')\n#plt.scatter(X, Y, color ='b')\n#plt.plot(X, Yhat, color ='k')\n#plt.show()\n\n\n#Fitting a Multiple Linear Regression\n#Z= df[['horsepower','curb-weight','engine-size','highway-mpg']]\n#X_train, X_test, y_train, y_test = train_test_split(Z, Y, test_size=0.3, random_state=0)\n#lm.fit(Z,Y)\n#Yhat=lm.predict(Z)\n#Visualising highway-mpg and price\n#width = 12\n#height = 10\n#plt.figure(figsize=(width, height))\n#sns.regplot(x=\"highway-mpg\", y=\"price\", data=df)\n#plt.ylim(0,)\n\n#plt.figure(figsize=(width, height))\n#sns.regplot(x=\"peak-rpm\", y=\"price\", data=df)\n#plt.ylim(0,)\n##lm.intercept_\n#lm.fit(Y.to_numpy().reshape(-1,1),Yhat)\n#residual = Yhat - lm.predict(Y.to_numpy().reshape(-1,1))\n\n#plt.scatter(Z, residual, color ='b')\n#plt.plot(Z, residual, color ='k')\n#plt.show()\n\n#print(df['price'])\n#df.describe(include=\"all\")\n#df.dtypes\n\n#Creating a residual plot with seaborn\n#width = 12\n#height = 10\n#plt.figure(figsize=(width, height))\n#sns.residplot(df['highway-mpg'], df['price'])\n#plt.show()\n\n#plt.figure(figsize=(width, height))\n#ax1 = sns.distplot(df['price'], hist=False, color=\"r\", label=\"Actual Value\")\n#sns.distplot(Yhat, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n#plt.title('Actual vs Fitted Values for Price')\n#plt.xlabel('Price (in dollars)')\n#plt.ylabel('Proportion of Cars')\n\n#plt.show()\n#plt.close()\n\n#sns.set(style=\"whitegrid\")\n#fig, ax = plt.subplots(1,2,figsize =(10,5))\n#sns.residplot(Y,Z, lowess=True, color=\"g\",ax=ax[0])\n#ax[0].set_xlim(0,2.5)\n#sns.regplot(x=Y,y=residual,lowess=True)\n#ax[1].set_xlim(0,2.5)\n\n#Visualising with Distribution plots\n#ax1=sns.distplot(Y,color='r',hist=False,label='Actual Value')\n#ax2=sns.distplot(Yhat, color='g',hist=False,label='fitted values')\n\n#Doing it with kdeplot\n#ax1=sns.kdeplot(Y,fill=True,color='r')\n#ax2=sns.kdeplot(X,fill=True, color='g')\n\n#Polynomial Regression\n#def PlotPolly(model, independent_variable, dependent_variabble, Name):\n    #x_new = np.linspace(15, 55, 100)\n    #y_new = model(x_new)\n\n    #plt.plot(independent_variable, dependent_variabble, '.', x_new, y_new, '-')\n    #plt.title('Polynomial Fit with Matplotlib for Price ~ Length')\n    #ax = plt.gca()\n    #ax.set_facecolor((0.898, 0.898, 0.898))\n    #fig = plt.gcf()\n    #plt.xlabel(Name)\n    #plt.ylabel('Price of Cars')\n\n    #plt.show()\n    #plt.close()\n    \n#x = df['highway-mpg']\n#y = df['price']\n# Here we use a polynomial of the 3rd order (cubic) \n#f = np.polyfit(x, y, 3)\n#p = np.poly1d(f)\n#print(p)\n#PlotPolly(p, x, y, 'highway-mpg')\n#np.polyfit(x, y, 3)\n#pr= PolynomialFeatures(degree=2,include_bias=False)\n#X_pol=pr.fit_transform(X,Y)\n\n#StandardScaler helps to normalise features due to the \"curse of dimensionality\"\n#Scaler=StandardScaler()\n#Scaler.fit(X,['horsepower'])\n#X_Scale=Scaler.transform(X,['horsepower'])\n\n#Using Pipelines for normalisation\n#Input=[('scale',StandardScaler()),('polynomial',PolynomialFeatures(degree=2)),('model',LinearRegression())]\n#pipe=Pipeline(Input)\n\n#Train the model\n#pipe.fit(X_train,y_train)\n#pipe.score(X_test,y_test)\n\n#Functions for plotting\ndef DistributionPlot(RedFunction, BlueFunction, RedName, BlueName, Title):\n    width = 12\n    height = 10\n    plt.figure(figsize=(width, height))\n\n    ax1 = sns.distplot(RedFunction, hist=False, color=\"r\", label=RedName)\n    ax2 = sns.distplot(BlueFunction, hist=False, color=\"b\", label=BlueName, ax=ax1)\n\n    plt.title(Title)\n    plt.xlabel('Price (in dollars)')\n    plt.ylabel('Proportion of Cars')\n\n    plt.show()\n    plt.close()\n    \n    def PollyPlot(xtrain, xtest, y_train, y_test, lr,poly_transform):\n        width = 12\n    height = 10\n    plt.figure(figsize=(width, height))\n    \n    \n    #training data \n    #testing data \n    # lr:  linear regression object \n    #poly_transform:  polynomial transformation object \n \n    xmax=max([x_train.values.max(), x_test.values.max()])\n\n    xmin=min([x_train.values.min(), x_test.values.min()])\n\n    x=np.arange(xmin, xmax, 0.1)\n\n\n    plt.plot(x_train, y_train, 'ro', label='Training Data')\n    plt.plot(x_test, y_test, 'go', label='Test Data')\n    plt.plot(x, lr.predict(poly_transform.fit_transform(x.reshape(-1, 1))), label='Predicted Function')\n    plt.ylim([-10000, 60000])\n    plt.ylabel('Price')\n    plt.legend()\n\ny_data = df['price']\nx_data=df.drop('price',axis=1)\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.10, random_state=1)\n\n\nprint(\"number of test samples :\", x_test.shape[0])\nprint(\"number of training samples:\",x_train.shape[0])\n\n#Cross-Validation Score\n\nlm.fit(x_train[['horsepower']], y_train)\nlm.score(x_test[['horsepower']], y_test)\nlm.score(x_train[['horsepower']], y_train)\nRcross = cross_val_score(lm, x_data[['horsepower']], y_data, cv=4)\nprint(\"The mean of the folds are\", Rcross.mean(), \"and the standard deviation is\" , Rcross.std())\n\n#We can use negative squared error as a score by setting the parameter  'scoring' metric to 'neg_mean_squared_error'.\n\n-1 * cross_val_score(lm,x_data[['horsepower']], y_data,cv=4,scoring='neg_mean_squared_error')\nyhat = cross_val_predict(lm,x_data[['horsepower']], y_data,cv=4)\nyhat[0:5]\n\n#Overfitting, Underfitting and Model Selection\nlr = LinearRegression()\nlr.fit(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_train)\nyhat_train = lr.predict(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\nyhat_train[0:5]\nyhat_test = lr.predict(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\nyhat_test[0:5]\n\n#Figure 1: Plot of predicted values using the training data compared to the actual values of the training data.\n\nTitle = 'Distribution  Plot of  Predicted Value Using Training Data vs Training Data Distribution'\nDistributionPlot(y_train, yhat_train, \"Actual Values (Train)\", \"Predicted Values (Train)\", Title)\nTitle='Distribution  Plot of  Predicted Value Using Test Data vs Data Distribution of Test Data'\nDistributionPlot(y_test,yhat_test,\"Actual Values (Test)\",\"Predicted Values (Test)\",Title)\n\n#<h4>Overfitting</h4>\n#Overfitting occurs when the model fits the noise, but not the underlying process. \n# Therefore, when testing your model using the test set, your model does not perform as well \n# since it is modelling noise, not the underlying process that generated the relationship. \n# Let's create a degree 5 polynomial model.\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.45, random_state=0)\npr = PolynomialFeatures(degree=5)\nx_train_pr = pr.fit_transform(x_train[['horsepower']])\nx_test_pr = pr.fit_transform(x_test[['horsepower']])\npr\n\n#Now, let's create a Linear Regression model \"poly\" and train it.\npoly = LinearRegression()\npoly.fit(x_train_pr, y_train)\nyhat = poly.predict(x_test_pr)\nyhat[0:5]\n\nprint(\"Predicted values:\", yhat[0:4])\nprint(\"True values:\", y_test[0:4].values)\n\n#We will use the function \"PollyPlot\" that we defined at the beginning of the lab to \n# display the training data, testing data, and the predicted function.\nPollyPlot(x_train[['horsepower']], x_test[['horsepower']], y_train, y_test, poly,pr)\n\n#Figure 3: A polynomial regression model where red dots represent training data, green dots \n# represent test data, and the blue line represents the model prediction.\npoly.score(x_train_pr, y_train)\npoly.score(x_test_pr, y_test)\n\n#We see the R^2 for the training data is 0.5567 while the R^2 on the test data was -29.87.  \n# The lower the R^2, the worse the model. A negative R^2 is a sign of overfitting.\n\nRsqu_test = []\n\norder = [1, 2, 3, 4]\nfor n in order:\n    pr = PolynomialFeatures(degree=n)\n    \n    x_train_pr = pr.fit_transform(x_train[['horsepower']])\n    \n    x_test_pr = pr.fit_transform(x_test[['horsepower']])    \n    \n    lr.fit(x_train_pr, y_train)\n    \n    Rsqu_test.append(lr.score(x_test_pr, y_test))\n\nplt.plot(order, Rsqu_test)\nplt.xlabel('order')\nplt.ylabel('R^2')\nplt.title('R^2 Using Test Data')\nplt.text(3, 0.75, 'Maximum R^2 ')  \n\n#We see the R^2 gradually increases until an order three polynomial is used. \n# Then, the R^2 dramatically decreases at an order four polynomial.\\\n    \ndef f(order, test_data):\n    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_data, random_state=0)\n    pr = PolynomialFeatures(degree=order)\n    x_train_pr = pr.fit_transform(x_train[['horsepower']])\n    x_test_pr = pr.fit_transform(x_test[['horsepower']])\n    poly = LinearRegression()\n    poly.fit(x_train_pr,y_train)\n    PollyPlot(x_train[['horsepower']], x_test[['horsepower']], y_train,y_test, poly, pr)  \\\n        \n#The following interface allows you to experiment with different polynomial orders and different amounts of data.\ninteract(f, order=(0, 6, 1), test_data=(0.05, 0.95, 0.05))\n\n#Part 3: Ridge Regression\n#Let's perform a degree two polynomial transformation on our data.\npr=PolynomialFeatures(degree=2)\nx_train_pr=pr.fit_transform(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])\nx_test_pr=pr.fit_transform(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])\n\nfrom sklearn.linear_model import Ridge\nRigeModel=Ridge(alpha=1)\nRigeModel.fit(x_train_pr, y_train)\nyhat = RigeModel.predict(x_test_pr)\n\n#Let's compare the first five predicted samples to our test set:\n\nprint('predicted:', yhat[0:4])\nprint('test set :', y_test[0:4].values)\n\n#We select the value of alpha that minimizes the test error. \n# To do so, we can use a for loop. We have also created a progress bar \n# to see how many iterations we have completed so far.\\\nfrom tqdm import tqdm\n\nRsqu_test = []\nRsqu_train = []\ndummy1 = []\nAlpha = 10 * np.array(range(0,1000))\npbar = tqdm(Alpha)\n\nfor alpha in pbar:\n    RigeModel = Ridge(alpha=alpha) \n    RigeModel.fit(x_train_pr, y_train)\n    test_score, train_score = RigeModel.score(x_test_pr, y_test), RigeModel.score(x_train_pr, y_train)\n    \n    pbar.set_postfix({\"Test Score\": test_score, \"Train Score\": train_score})\n\n    Rsqu_test.append(test_score)\n    Rsqu_train.append(train_score)\n    \n#We can plot out the value of R^2 for different alphas:\nwidth = 12\nheight = 10\nplt.figure(figsize=(width, height))\n\nplt.plot(Alpha,Rsqu_test, label='validation data  ')\nplt.plot(Alpha,Rsqu_train, 'r', label='training Data ')\nplt.xlabel('alpha')\nplt.ylabel('R^2')\nplt.legend()\n\n#Part 4: Grid Search\nfrom sklearn.model_selection import GridSearchCV\n#We create a dictionary of parameter values:\nparameters1= [{'alpha': [0.001,0.1,1, 10, 100, 1000, 10000, 100000, 100000]}]\nparameters1\n#Create a Ridge regression object:\nRR=Ridge()\nRR\n#Create a ridge grid search object:\nGrid1 = GridSearchCV(RR, parameters1,cv=4)\nGrid1.fit(x_data[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_data)\n#The object finds the best parameter values on the validation data. \n# We can obtain the estimator with the best parameters and assign it to the variable BestRR as follows:\n\nBestRR=Grid1.best_estimator_\nBestRR\n#We now test our model on the test data:\n\nBestRR.score(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_test)\n\n\n\n\n\n\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "3f0af58d842f97082c0579486276539607f48c8f64052199aef418a68a12ec3b"
        },
        "kernelspec": {
            "display_name": "Python 3.9",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}